{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e33edd30-8e4b-4d27-b37f-ac6208f2fc23",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "## Builder Data Loading App\n",
    "\n",
    "This script loads Excel or CSV files from the Advana External Data Load (EDL) site into a specified table. The data is then available for use within the Advana platform, accessible via SQL queries or a Python notebook in Databricks for further processing. Only builder data is supported. Enterprise data is not supported with this script.\n",
    "\n",
    "The application includes robust input checks and provides feedback for errors. If an issue prevents the data load from completing, an error will be displayed, and the program will cease execution.\n",
    "\n",
    "***\n",
    "\n",
    "### How to Use\n",
    "\n",
    "1.  **Upload** your CSV or Excel file to the Advana External Data Load (EDL) site under the `Builder Data` location.\n",
    "    * Only `.xlsx` (Excel) or `.csv` files are supported.\n",
    "\n",
    "2.  **Wait** until `COMPLETED` appears in the `Status` field on the Advana EDL site. This may take a while. You may need to refresh the page to see the status update.\n",
    "\n",
    "3.  **Copy** the entire `Key` field for the file you uploaded to Advana EDL and paste it into the `Key (from Advana EDL)` field.\n",
    "    * Can't see the `Key (from Advana EDL)` field? This means the widgets haven't loaded. Press the `Run all` button at the top right. You will then see widgets at the top of the script where you can enter the key.\n",
    "\n",
    "4.  **Select** either `Overwrite` or `Append` based on your needs.\n",
    "    * **Warning:** Selecting `Overwrite` will delete all previous data in the table.\n",
    "\n",
    "5.  **Enter** the number of rows you want to skip. This is useful for skipping empty or title rows in your file.\n",
    "\n",
    "6.  **Enter** the desired table name for your data.\n",
    "    * Ensure you enter the correct table name before running the application.\n",
    "\n",
    "7.  **Click** `Run all`.\n",
    "\n",
    "***\n",
    "\n",
    "### Results\n",
    "\n",
    "1)  **Success:** Every cell of code will run and the last cell will have a ✅ green check mark.\n",
    "    * The key and target schema and table name will be listed.\n",
    "    * The data is now ready for further processing.\n",
    "2)  **Failure:** There will be a cell that failed.\n",
    "    * Failure can easily be seen by a red rectangle on the right side. Click the red rectangle to be taken to the cell and view the error message.\n",
    "    * The failed cell can also be found by scrolling to the cell.\n",
    "    * Follow the instructions of the error message to resolve the issue.\n",
    "\n",
    "***\n",
    "\n",
    "### Notes\n",
    "\n",
    "* This program only reads the _first sheet_ of an Excel file. Make sure your data is on the first sheet.\n",
    "* The program automatically removes any empty columns.\n",
    "* If `Skip Rows` is not specified, the script will not skip any rows.\n",
    "* Column names are automatically renamed to a Databricks-compatible format. Specifically, the program:\n",
    "    * Changes the name to all uppercase.\n",
    "    * Replaces spaces, slashes, and periods with underscores.\n",
    "    * Removes other invalid characters as defined by [Databricks](https://docs.databricks.com/aws/en/sql/language-manual/sql-ref-names).\n",
    "* Excel files are processed in-memory while CSV files have a distributed processing.\n",
    "    * Under normal use cases, there should not be an issue with processing excel files in-memory.  \n",
    "\n",
    "***\n",
    "\n",
    "Created by Noel Mrowiec  \n",
    "noel.j.mrowiec2.ctr@us.navy.mil  \n",
    "10/1/202"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4209f35e-5a16-4b1b-9c1b-b68ba13bf5e4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python interpreter will be restarted.\n",
      "Looking in indexes: http://mirror.vdms.uot.local/simple/, https://nexus.advana.data.mil/repository/pypi-all/simple/\n",
      "Requirement already satisfied: openpyxl in /local_disk0/.ephemeral_nfs/envs/pythonEnv-0b75534e-fb53-4662-bc0c-15643468ecf3/lib/python3.9/site-packages (3.1.5)\n",
      "Requirement already satisfied: et-xmlfile in /local_disk0/.ephemeral_nfs/envs/pythonEnv-0b75534e-fb53-4662-bc0c-15643468ecf3/lib/python3.9/site-packages (from openpyxl) (2.0.0)\n",
      "Python interpreter will be restarted.\n"
     ]
    }
   ],
   "source": [
    "pip install openpyxl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e3f65355-c4c8-4d18-941b-b6e49f30c9c0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def install_widgets():\n",
    "  dbutils.widgets.text('Key (from Advana EDL)', \"\")\n",
    "  dbutils.widgets.text('Target Table Name','')\n",
    "  dbutils.widgets.dropdown('Target Schema', 'jup_fmc_restricted_workspace', ['jup_fmc_restricted_workspace'])\n",
    "  dbutils.widgets.dropdown('Overwrite or Append', \"Append\", ['Overwrite', 'Append'])\n",
    "  dbutils.widgets.text('Skip Rows', '0')\n",
    "  dbutils.widgets.dropdown('Region','us-gov-west-1', ['us-gov-west-1'])\n",
    "\n",
    "install_widgets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "03b1f594-2b41-4dd9-866d-3f507e5deb8b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os \n",
    "\n",
    "def is_valid_extension(filename : str) -> bool:\n",
    "    name, extension = os.path.splitext(filename)\n",
    "    if extension.lower() in ['.xlsx']:\n",
    "        return True\n",
    "    elif extension.lower() == '.csv':\n",
    "        return True\n",
    "    else: \n",
    "        return False\n",
    "    \n",
    "def is_correct_cluster(key: str) -> bool:\n",
    "    if \"non-sensitive\" in key:\n",
    "        return False\n",
    "    else:\n",
    "        return True\n",
    "\n",
    "def get_s3_path_from_key(key : str) -> str:\n",
    "    # Check that the key is Excel or CSV file and not on the incorrect cluster\n",
    "    if not is_valid_extension(key):\n",
    "        raise Exception(f\"Invalid key: {key}.\\nKey must have a valid extension (.csv, .xls, or .xlsx.)\")\n",
    "    elif not is_correct_cluster(key):\n",
    "        raise Exception(f\"Invalid cluster: {key}.\\nData located on the incorrect cluster. Make sure you upload data to jup-fmc-r.\")\n",
    "\n",
    "    idx_path_start = key.find(\"builder\")\n",
    "    if idx_path_start < 0:\n",
    "        # doens't have builder keyword, so either fix the issue, or prompt for a new key\n",
    "        raise Exception(f\"Invalid key: {key}.\\nPlease check the key field.\")\n",
    "    \n",
    "    s3_path = key[idx_path_start:]\n",
    "    print(f's3 path: {s3_path}')\n",
    "\n",
    "    return s3_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "81b77cd3-7701-4ea6-8217-7e95affeccb8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.databricks.v1+bamboolib_hint": "{\"pd.DataFrames\": [], \"version\": \"0.0.1\"}",
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "def clean_names(column_name:str, to_upper:bool=True):\n",
    "    \"\"\"\n",
    "    Cleans a string to be used as a valid and standardized column name.\n",
    "    It removes invalid characters, replaces spaces with underscores,\n",
    "    truncates to a maximum length, and converts the name to uppercase.\n",
    "    \n",
    "    Args:\n",
    "        column_name (str): The original column name as a string.\n",
    "        to_upper (bool): Makes the name uppercase, if true.\n",
    "\n",
    "    Returns:\n",
    "        str: The cleaned and standardized column name.\n",
    "    \"\"\"\n",
    "    column_name = column_name.strip()\n",
    "\n",
    "    if len(column_name) > 255: \n",
    "        # max of 255 characters\n",
    "        column_name = column_name[:255]\n",
    "\n",
    "    cleaned_name = re.sub(r'[,;{}()\\n\\t=\\x00-\\x1F\\x7F]', '', column_name)\n",
    "    cleaned_name = re.sub(r'[ ./-]', '_', cleaned_name)\n",
    "    \n",
    "    cleaned_name = cleaned_name.upper() if to_upper else cleaned_name\n",
    "    \n",
    "    return cleaned_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b4c3451e-7bc4-46c5-a17c-34070e1861ab",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "from pyspark.sql import SparkSession\n",
    "from botocore.exceptions import ClientError\n",
    "\n",
    "def get_data_from_s3(s3_key:str, region:str, target_schema:str, rows_to_skip:int=0, file_type:str=\"Excel\", s3_bucket='advana-restricted-data-zone'):\n",
    "    \"\"\"\n",
    "    Reads a file (Excel or CSV) from an S3 bucket, cleans it using Pandas,\n",
    "    and converts it into a Spark DataFrame for further processing.\n",
    "\n",
    "    This function is designed for scenarios where cluster libraries cannot be\n",
    "    installed, such as when dealing with small to medium-sized files that can be\n",
    "    read into the driver's memory. The process involves:\n",
    "    1. Reading the file directly from S3 into memory using `boto3`.\n",
    "    2. Using `pandas` to load the in-memory byte stream into a DataFrame.\n",
    "    3. Performing basic data cleaning, such as dropping empty columns and\n",
    "       standardizing column names.\n",
    "    4. Converting the cleaned Pandas DataFrame into a PySpark DataFrame to\n",
    "       leverage distributed processing capabilities for subsequent steps.\n",
    "\n",
    "    Args:\n",
    "        s3_key (str): The full path to the file within the S3 bucket.\n",
    "        region (str): The AWS region where the S3 bucket is located.\n",
    "        rows_to_skip (int): The number of rows to skip at the beginning of the file.\n",
    "            This is useful for files with headers or metadata before the actual data.\n",
    "        file_type (str): The type of file to be processed. Supports \"Excel\" or \"CSV\".\n",
    "        s3_bucket (str): The name of the S3 bucket. Defaults to 'advana-restricted-data-zone'.\n",
    "\n",
    "    Returns:\n",
    "        pyspark.sql.DataFrame: A PySpark DataFrame containing the cleaned data.\n",
    "\n",
    "    Raises:\n",
    "        Exception: If the specified file type is not 'Excel' or 'CSV'.\n",
    "        ClientError: If there's an AWS-related error, such as 'NoSuchKey', which\n",
    "            indicates the file was not found.\n",
    "    \"\"\"\n",
    "\n",
    "    print(f\"Starting job for S3 file: s3://{s3_bucket}/{s3_key}\")\n",
    "    \n",
    "    try:\n",
    "        # Connect to the S3 service\n",
    "        s3 = boto3.client('s3', region_name=region)\n",
    "        \n",
    "        # Get the file object from S3\n",
    "        obj = s3.get_object(Bucket=s3_bucket, Key=s3_key)\n",
    "        \n",
    "        # Read the entire file's byte stream into memory.\n",
    "        file_content = obj['Body'].read()\n",
    "        \n",
    "        # Now, pass the in-memory bytes object to pandas.\n",
    "\n",
    "        pandas_df = None\n",
    "        if file_type.lower() == \"excel\":\n",
    "            pandas_df = pd.read_excel(file_content, header=rows_to_skip, sheet_name=0, engine='openpyxl') \n",
    "        elif file_type.lower() == \"csv\":\n",
    "            pandas_df = pd.read_csv(io.BytesIO(file_content), skiprows=rows_to_skip, engine=\"python\")\n",
    "        else:  \n",
    "            raise Exception(\"Wrong file type. Only Excel or CSV files can be loaded\")\n",
    "        \n",
    "        #print(\"Successfully read Excel file into a Pandas DataFrame.\")\n",
    "        \n",
    "        return pandas_df\n",
    "\n",
    "    except ClientError as e:\n",
    "        if e.response['Error']['Code'] == 'NoSuchKey':\n",
    "            raise Exception(f'''This error can be caused by one of the following issues:\\n\n",
    "    Incorrect Key: The provided key is incorrect. Please verify that it exactly matches the key from the Advana EDL site.\n",
    "    Incorrect Cluster: The data was uploaded to the wrong cluster. Ensure the cluster selected during the upload contains the {target_schema} schema.\n",
    "    Server Overload: The Advana EDL server may be temporarily overloaded due to high traffic. Please wait a few minutes and try again. (Note: during high traffic, it can take hours for the data to be loaded.)''')\n",
    "\n",
    "        else:\n",
    "            # For any other client error, re-raise the exception\n",
    "            raise Exception(\"An unexpected error occurred {e}.\")\n",
    "                \n",
    "    except Exception as e:\n",
    "        raise Exception(f\"An unexpected error occurred {e}.\")\n",
    "\n",
    "def clean_and_prep_dataframe(pandas_df):\n",
    "    try:\n",
    "        # Perform cleaning in Pandas.\n",
    "        # Drop null/blank columns and rename columns\n",
    "        pandas_df_cleaned = pandas_df.dropna(axis='columns', how='all')\n",
    "        pandas_df_cleaned = pandas_df_cleaned.rename(columns=clean_names)\n",
    "        #print(\"Cleaned Pandas DataFrame by dropping empty columns.\")\n",
    "        print(f'Columns of loaded data:\\n{pandas_df_cleaned.columns}')\n",
    "\n",
    "        # Convert the cleaned Pandas DataFrame to a PySpark DataFrame.\n",
    "        # This step parallelizes the data for distributed writing.\n",
    "        spark_df = spark.createDataFrame(pandas_df_cleaned)\n",
    "    except TypeError as e:\n",
    "        raise TypeError(\"Data type is incorrect. Recheck your data. Check that your data has column names. Ensure you are skipping the correct number of rows.\")\n",
    "\n",
    "\n",
    "    return spark_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a25afde2-6259-4872-9843-74969aa621f5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def to_database(spark_df, mode: str, target_schema: str, target_table: str):\n",
    "    # Overwrite or append the Delta table using Spark.\n",
    "    if mode == \"Append\":\n",
    "        (spark_df.write\n",
    "            .format('delta')\n",
    "            .mode(\"append\")\n",
    "            .option(\"delta.columnMapping.mode\",\"name\")\n",
    "            .saveAsTable(f\"{target_schema}.{target_table}\")\n",
    "        )\n",
    "        print(f\"Successfully appended Delta Table: {target_schema}.{target_table}\")\n",
    "    elif mode == \"Overwrite\":\n",
    "        (spark_df.write\n",
    "            .format('delta')\n",
    "            .mode(\"overwrite\")\n",
    "            .option(\"overwriteSchema\", \"true\")\n",
    "            .saveAsTable(f\"{target_schema}.{target_table}\")\n",
    "        )\n",
    "        print(f\"Successfully overwrote Delta Table: {target_schema}.{target_table}\")\n",
    "    else:\n",
    "        raise Exception(\"Selection error: Only overwrite or append allowed.\")\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "33154c01-ccee-46e2-b78f-8745f98f7951",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n",
       "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)\n",
       "\u001b[0;32m<command-3432481934381189>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n",
       "\u001b[1;32m      1\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdbutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwidgets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Key (from Advana EDL)'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mraise\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Error: `Key (from Advana EDL)` must not be blank\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mtarget_tablename\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdbutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwidgets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Target Table Name'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[1;32m      4\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtarget_tablename\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mraise\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Error: `Target Table Name` must not be blank\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[1;32m      5\u001b[0m \u001b[0mtarget_tablename\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclean_names\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget_tablename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mto_upper\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\n",
       "\u001b[0;31mException\u001b[0m: Error: `Key (from Advana EDL)` must not be blank"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)\n\u001b[0;32m<command-3432481934381189>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdbutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwidgets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Key (from Advana EDL)'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mraise\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Error: `Key (from Advana EDL)` must not be blank\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mtarget_tablename\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdbutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwidgets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Target Table Name'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtarget_tablename\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mraise\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Error: `Target Table Name` must not be blank\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtarget_tablename\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclean_names\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget_tablename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mto_upper\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\n\u001b[0;31mException\u001b[0m: Error: `Key (from Advana EDL)` must not be blank",
       "errorSummary": "<span class='ansi-red-fg'>Exception</span>: Error: `Key (from Advana EDL)` must not be blank",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "key=dbutils.widgets.get('Key (from Advana EDL)').strip()\n",
    "if not key: raise Exception(\"Error: `Key (from Advana EDL)` must not be blank\")\n",
    "target_tablename=dbutils.widgets.get('Target Table Name')\n",
    "if not target_tablename: raise Exception(\"Error: `Target Table Name` must not be blank\")\n",
    "target_tablename=clean_names(target_tablename, to_upper=False)\n",
    "target_schema=dbutils.widgets.get('Target Schema')\n",
    "ua=dbutils.widgets.get('Overwrite or Append')\n",
    "skiprows=dbutils.widgets.get('Skip Rows')\n",
    "skiprows = 0 if not skiprows else int(skiprows)\n",
    "region=dbutils.widgets.get('Region')\n",
    "\n",
    "path = get_s3_path_from_key(key)\n",
    "print(f'key={key}, target_tablename={target_tablename}, target_schema={target_schema},ua={ua}, region={region}, skiprows={skiprows}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "75d27345-5f53-42e8-a73f-dbf28111d416",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": []
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "Command skipped",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "pandas_df = get_data_from_s3(path, region, target_schema, rows_to_skip=skiprows, file_type=\"Excel\")\n",
    "spark_df = clean_and_prep_dataframe(pandas_df)\n",
    "to_database(spark_df, mode=ua, target_schema=target_schema, target_table=target_tablename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4a78ea3b-bc2e-4be3-bb2c-e6cf4e481461",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": []
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "Command skipped",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "dbutils.widgets.removeAll()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7070d46d-7e56-4327-b4cf-86fd280a78d4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": []
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "Command skipped",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "install_widgets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0cbbbed8-56f2-4646-82a7-ea4a60dee9d2",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1761070800569}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": []
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "Command skipped",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df=sql(f\"select * from {target_schema}.{target_tablename}\")\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c53f8279-d16a-43e9-8cc6-ac40eee2b27f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": []
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "Command skipped",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(f\"\"\"✅ File was successfully loaded to table:\n",
    "      Key: {key} \n",
    "      Schema and table name:  {target_schema}.{target_tablename}\"\"\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 5507392346735877,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Advana Builder EDL",
   "widgets": {
    "Key (from Advana EDL)": {
     "currentValue": "",
     "nuid": "04a47dc6-9eb0-4d9f-88d7-ebde103d290a",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "",
      "label": null,
      "name": "Key (from Advana EDL)",
      "options": {
       "validationRegex": null,
       "widgetDisplayType": "Text"
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "defaultValue": "",
      "label": null,
      "name": "Key (from Advana EDL)",
      "options": {
       "autoCreated": null,
       "validationRegex": null,
       "widgetType": "text"
      },
      "widgetType": "text"
     }
    },
    "Overwrite or Append": {
     "currentValue": "Append",
     "nuid": "aead0e76-34ef-4ef7-b8cf-b3ddf0b5b172",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "Append",
      "label": null,
      "name": "Overwrite or Append",
      "options": {
       "choices": [
        "Overwrite",
        "Append"
       ],
       "fixedDomain": true,
       "multiselect": false,
       "widgetDisplayType": "Dropdown"
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "defaultValue": "Append",
      "label": null,
      "name": "Overwrite or Append",
      "options": {
       "autoCreated": null,
       "choices": [
        "Overwrite",
        "Append"
       ],
       "widgetType": "dropdown"
      },
      "widgetType": "dropdown"
     }
    },
    "Region": {
     "currentValue": "us-gov-west-1",
     "nuid": "2001671f-f90e-4c58-83d3-7805734dd68e",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "us-gov-west-1",
      "label": null,
      "name": "Region",
      "options": {
       "choices": [
        "us-gov-west-1"
       ],
       "fixedDomain": true,
       "multiselect": false,
       "widgetDisplayType": "Dropdown"
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "defaultValue": "us-gov-west-1",
      "label": null,
      "name": "Region",
      "options": {
       "autoCreated": null,
       "choices": [
        "us-gov-west-1"
       ],
       "widgetType": "dropdown"
      },
      "widgetType": "dropdown"
     }
    },
    "Skip Rows": {
     "currentValue": "0",
     "nuid": "cbe1156f-f623-413d-bd94-b3945ba7b61d",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "0",
      "label": null,
      "name": "Skip Rows",
      "options": {
       "validationRegex": null,
       "widgetDisplayType": "Text"
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "defaultValue": "0",
      "label": null,
      "name": "Skip Rows",
      "options": {
       "autoCreated": null,
       "validationRegex": null,
       "widgetType": "text"
      },
      "widgetType": "text"
     }
    },
    "Target Schema": {
     "currentValue": "jup_fmc_restricted_workspace",
     "nuid": "b7468299-7b47-453c-8e75-37151364d274",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "jup_fmc_restricted_workspace",
      "label": null,
      "name": "Target Schema",
      "options": {
       "choices": [
        "jup_fmc_restricted_workspace"
       ],
       "fixedDomain": true,
       "multiselect": false,
       "widgetDisplayType": "Dropdown"
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "defaultValue": "jup_fmc_restricted_workspace",
      "label": null,
      "name": "Target Schema",
      "options": {
       "autoCreated": null,
       "choices": [
        "jup_fmc_restricted_workspace"
       ],
       "widgetType": "dropdown"
      },
      "widgetType": "dropdown"
     }
    },
    "Target Table Name": {
     "currentValue": "",
     "nuid": "3a30ac59-fc30-4f89-ac44-6600bc9a047e",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "",
      "label": null,
      "name": "Target Table Name",
      "options": {
       "validationRegex": null,
       "widgetDisplayType": "Text"
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "defaultValue": "",
      "label": null,
      "name": "Target Table Name",
      "options": {
       "autoCreated": null,
       "validationRegex": null,
       "widgetType": "text"
      },
      "widgetType": "text"
     }
    }
   }
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
